I am 2 of 8: 2 on x3200c0s19b1n0
I am 0 of 8: 0 on x3200c0s19b1n0
I am 3 of 8: 3 on x3200c0s19b1n0
I am 1 of 8: 1 on x3200c0s19b1n0
I am 7 of 8: 3 on x3200c0s1b0n0
I am 5 of 8: 1 on x3200c0s1b0n0
I am 4 of 8: 0 on x3200c0s1b0n0
I am 6 of 8: 2 on x3200c0s1b0n0
x3200c0s19b1n0:190244:190244 [0] NCCL INFO Bootstrap : Using bond0:10.140.48.179<0>
x3200c0s19b1n0:190244:190244 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
x3200c0s19b1n0:190244:190244 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
x3200c0s19b1n0:190244:190244 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.20.5+cuda12.4
x3200c0s19b1n0:190246:190246 [1] NCCL INFO cudaDriverVersion 12020
x3200c0s19b1n0:190246:190246 [1] NCCL INFO Bootstrap : Using bond0:10.140.48.179<0>
x3200c0s19b1n0:190246:190246 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
x3200c0s19b1n0:190246:190246 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
x3200c0s19b1n0:190246:190465 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s19b1n0:190246:190465 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s19b1n0:190246:190465 [1] NCCL INFO NET/OFI Using Libfabric version 1.15
x3200c0s19b1n0:190246:190465 [1] NCCL INFO NET/OFI Using CUDA driver version 12020
x3200c0s19b1n0:190246:190465 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
x3200c0s19b1n0:190246:190465 [1] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s19b1n0:190246:190465 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190246:190465 [1] NCCL INFO Using non-device net plugin version 0
x3200c0s19b1n0:190246:190465 [1] NCCL INFO Using network AWS Libfabric
x3200c0s19b1n0:190246:190465 [1] NCCL INFO DMA-BUF is available on GPU device 1
x3200c0s19b1n0:190246:190465 [1] NCCL INFO comm 0x559a428825d0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 46000 commId 0x5f4dce13ead548e7 - Init START
x3200c0s19b1n0:190246:190465 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190246:190465 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190246:190465 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190246:190465 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190246:190465 [1] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s19b1n0:190246:190465 [1] NCCL INFO Setting affinity for GPU 1 to ff0000
x3200c0s19b1n0:190246:190465 [1] NCCL INFO NVLS multicast support is notx3200c0s1b0n0:199895:199895 [1] NCCL INFO cudaDriverVersion 12020
x3200c0s1b0n0:199895:199895 [1] NCCL INFO Bootstrap : Using bond0:10.140.48.173<0>
x3200c0s1b0n0:199895:199895 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
x3200c0s1b0n0:199895:199895 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
x3200c0s1b0n0:199895:200115 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s1b0n0:199895:200115 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s1b0n0:199895:200115 [1] NCCL INFO NET/OFI Using Libfabric version 1.15
x3200c0s1b0n0:199895:200115 [1] NCCL INFO NET/OFI Using CUDA driver version 12020
x3200c0s1b0n0:199895:200115 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
x3200c0s1b0n0:199895:200115 [1] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s1b0n0:199895:200115 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199895:200115 [1] NCCL INFO Using non-device net plugin version 0
x3200c0s1b0n0:199895:200115 [1] NCCL INFO Using network AWS Libfabric
x3200c0s1b0n0:199895:200115 [1] NCCL INFO DMA-BUF is available on GPU device 1
x3200c0s1b0n0:199895:200115 [1] NCCL INFO comm 0x55a7d6c40580 rank 5 nranks 8 cudaDev 1 nvmlDev 1 busId 46000 commId 0x5f4dce13ead548e7 - Init START
x3200c0s1b0n0:199895:200115 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199895:200115 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199895:200115 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199895:200115 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199895:200115 [1] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s1b0n0:199895:200115 [1] NCCL INFO Setting affinity for GPU 1 to ff0000
x3200c0s1b0n0:199895:200115 [1] NCCL INFO NVLS multicast support is not available on dev 1
x3200c0s1b0n0:199895:200115 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
x3200c0s1b0n0:199895:200115 [1] NCCL INFO comm 0x55a7d6c40580 rank 5 nRanks 8 nNodes 2 localRanks 4 localRank 1 MNNVL 0
x3200c0s1b0n0:199895:200115 [1] NCCL INFO Trees [0] 6/-1/-1->5->1 [1] -1/-1/-1->5->4 [2] 6/1/-1->5->-1 [3] -1/-1/-1->5->4
x3200c0s1b0n0:199895:200115 [1] NCCL INFO P2P Chunksize set to 131072
x3200c0s1b0n0:199895:200120 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199895:200115 [1] NCCL INFO Channel 01/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/3/GDRDMA
x3200c0s1b0n0:199895:200120 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199895:200115 [1] NCCL INFO Channel 03/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/3/GDRDMA
x3200c0s1b0n0:199895:200120 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199895:200115 [1] NCCL INFO Channel 00/0 : 5[1] -> 1[1] [send] via NET/AWS Libfabric/3/GDRDMA
x3200c0s1b0n0:199895:200120 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199895:200115 [1] NCCL INFO Channel 02/0 : 5[1] -> 1[1] [send] via NET/AWS Libfabric/3/GDRDMA
x3200c0s1b0n0:199895:200115 [1] NCCL INFO Channel 01/0 : 5[1] -> 4[0] via P2P/CUMEM/read
x3200c0s1b0n0:199895:200115 [1] NCCL INFO Channel 03/0 : 5[1] -> 4[0] via P2P/CUMEM/read
x3200c0s1b0n0:199895:200115 [1] NCCL INFO Connected all rings
x3200c0s1b0n0:199895:200115 [1] NCCL INFO Channel 00/0 : 5[1] -> 6[2] via P2P/CUMEM/read
x3200c0s1b0n0:199895:200115 [1] NCCL INFO Channel 02/0 : 5[1] -> 6[2] via P2P/CUMEM/read
x3200c0s1b0n0:199895:200120 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199895:200115 [1] NCCL INFO Channel 00/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/3/GDRDMA
x3200c0s1b0n0:199895:200120 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199895:200115 [1] NCCL INFO Channel 02/0 : 1[1] -> 5[1] [receive] via NET/AWS Li available on dev 1
x3200c0s19b1n0:190246:190465 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
x3200c0s19b1n0:190246:190465 [1] NCCL INFO comm 0x559a428825d0 rank 1 nRanks 8 nNodes 2 localRanks 4 localRank 1 MNNVL 0
x3200c0s19b1n0:190246:190465 [1] NCCL INFO Trees [0] 2/5/-1->1->-1 [1] -1/-1/-1->1->0 [2] 2/-1/-1->1->5 [3] -1/-1/-1->1->0
x3200c0s19b1n0:190246:190465 [1] NCCL INFO P2P Chunksize set to 131072
x3200c0s19b1n0:190246:190472 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190246:190465 [1] NCCL INFO Channel 00/0 : 5[1] -> 1[1] [receive] via NET/AWS Libfabric/3/GDRDMA
x3200c0s19b1n0:190246:190472 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190246:190465 [1] NCCL INFO Channel 02/0 : 5[1] -> 1[1] [receive] via NET/AWS Libfabric/3/GDRDMA
x3200c0s19b1n0:190246:190472 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190246:190465 [1] NCCL INFO Channel 01/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/3/GDRDMA
x3200c0s19b1n0:190246:190472 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190246:190465 [1] NCCL INFO Channel 03/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/3/GDRDMA
x3200c0s19b1n0:190246:190465 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read
x3200c0s19b1n0:190246:190465 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM/read
x3200c0s19b1n0:190246:190465 [1] NCCL INFO Connected all rings
x3200c0s19b1n0:190246:190465 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM/read
x3200c0s19b1n0:190246:190465 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM/read
x3200c0s19b1n0:190246:190472 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190246:190465 [1] NCCL INFO Channel 00/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/3/GDRDMA
x3200c0s19b1n0:190246:190472 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190246:190465 [1] NCCL INFO Channel 02/0bf :x3200c0s19b1n0:190245:190245 [3] NCCL INFO cudaDriverVersion 12020
x3200c0s19b1n0:190245:190245 [3] NCCL INFO Bootstrap : Using bond0:10.140.48.179<0>
x3200c0s19b1n0:190245:190245 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
x3200c0s19b1n0:190245:190245 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
x3200c0s19b1n0:190245:190466 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s19b1n0:190245:190466 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s19b1n0:190245:190466 [3] NCCL INFO NET/OFI Using Libfabric version 1.15
x3200c0s19b1n0:190245:190466 [3] NCCL INFO NET/OFI Using CUDA driver version 12020
x3200c0s19b1n0:190245:190466 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
x3200c0s19b1n0:190245:190466 [3] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s19b1n0:190245:190466 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190245:190466 [3] NCCL INFO Using non-device net plugin version 0
x3200c0s19b1n0:190245:190466 [3] NCCL INFO Using network AWS Libfabric
x3200c0s19b1n0:190245:190466 [3] NCCL INFO DMA-BUF is available on GPU device 3
x3200c0s19b1n0:190245:190466 [3] NCCL INFO comm 0x5630808a4380 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId c7000 commId 0x5f4dce13ead548e7 - Init START
x3200c0s19b1n0:190245:190466 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190245:190466 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190245:190466 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190245:190466 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190245:190466 [3] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s19b1n0:190245:190466 [3] NCCL INFO NVLS multicast support is not available on dev 3
x3200c0s19b1n0:190245:190466 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
x3200c0s19b1n0:190245:190466 [3] NCCL INFO comm 0x5630808a4380 rank 3 nRanks 8 nNodes 2 localRanks 4 localRank 3 MNNVL 0
x3200c0s19b1n0:190245:190466 [3] NCCL INFO Trees [0] 0/-1/-1->3->2 [1] 2/7/-1->3->-1 [2] 0/-1/-1->3->2 [3] 2/-1/-1->3->7
x3200c0s19b1n0:190245:190466 [3] NCCL INFO P2P Chunksize set to 131072
x3200c0s19b1n0:190245:190468 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190245:190466 [3] NCCL INFO Channel 01/0 : 7[3] -> 3[3] [receive] via NET/AWS Libfabric/1/GDRDMA
x3200c0s19b1n0:190245:190468 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190245:190466 [3] NCCL INFO Channel 03/0 : 7[3] -> 3[3] [receive] via NET/AWS Libfabric/1/GDRDMA
x3200c0s19b1n0:190245:190468 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190245:190466 [3] NCCL INFO Channel 00/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/1/GDRDMA
x3200c0s19b1n0:190245:190468 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190245:190466 [3] NCCL INFO Channel 02/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/1/GDRDMA
x3200c0s19b1n0:190245:190466 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM/read
x3200c0s19b1n0:190245:190466 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM/read
x3200c0s19b1n0:190245:190466 [3] NCCL INFO Connected all rings
x3200c0s19b1n0:190245:190468 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190245:190466 [3] NCCL INFO Channel 01/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/1/GDRDMA
x3200c0s19b1n0:190245:190468 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190245:190466 [3] NCCL INFO Channel 03/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/1/GDRDMA
x3200c0s19b1n0:190245:190466 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM/read
x3200c0s19b1n0:190245:190466 [3] NCCL INFO Channel 02/0 : 3[3] -> 0[0] via P2P/CUMEM/read
x3200c0s19b1n0:190245:190466x3200c0s1b0n0:199893:199893 [3] NCCL INFO cudaDriverVersion 12020
x3200c0s1b0n0:199893:199893 [3] NCCL INFO Bootstrap : Using bond0:10.140.48.173<0>
x3200c0s1b0n0:199893:199893 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
x3200c0s1b0n0:199893:199893 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
x3200c0s1b0n0:199893:200114 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s1b0n0:199893:200114 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s1b0n0:199893:200114 [3] NCCL INFO NET/OFI Using Libfabric version 1.15
x3200c0s1b0n0:199893:200114 [3] NCCL INFO NET/OFI Using CUDA driver version 12020
x3200c0s1b0n0:199893:200114 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
x3200c0s1b0n0:199893:200114 [3] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s1b0n0:199893:200114 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199893:200114 [3] NCCL INFO Using non-device net plugin version 0
x3200c0s1b0n0:199893:200114 [3] NCCL INFO Using network AWS Libfabric
x3200c0s1b0n0:199893:200114 [3] NCCL INFO DMA-BUF is available on GPU device 3
x3200c0s1b0n0:199893:200114 [3] NCCL INFO comm 0x55da73ee82e0 rank 7 nranks 8 cudaDev 3 nvmlDev 3 busId c7000 commId 0x5f4dce13ead548e7 - Init START
x3200c0s1b0n0:199893:200114 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199893:200114 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199893:200114 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199893:200114 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199893:200114 [3] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s1b0n0:199893:200114 [3] NCCL INFO NVLS multicast support is not available on dev 3
x3200c0s1b0n0:199893:200114 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1. [
x3200c0s1b0n0:199893:200114 [3] NCCL INFO comm 0x55da73ee82e0 rank 7 nRanks 8 nNodes 2 localRanks 4 localRank 3 MNNVL 0
x3200c0s1b0n0:199893:200114 [3] NCCL INFO Trees [0] 4/-1/-1->7->6 [1] 6/-1/-1->7->3 [2] 4/-1/-1->7->6 [3] 6/3/-1->7->-1
x3200c0s1b0n0:199893:200114 [3] NCCL INFO P2P Chunksize set to 131072
x3200c0s1b0n0:199893:200117 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199893:200114 [3] NCCL INFO Channel 00/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/1/GDRDMA
x3200c0s1b0n0:199893:200117 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199893:200114 [3] NCCL INFO Channel 02/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/1/GDRDMA
x3200c0s1b0n0:199893:200117 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199893:200114 [3] NCCL INFO Channel 01/0 : 7[3] -> 3[3] [send] via NET/AWS Libfabric/1/GDRDMA
x3200c0s1b0n0:199893:200117 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199893:200114 [3] NCCL INFO Channel 03/0 : 7[3] -> 3[3] [send] via NET/AWS Libfabric/1/GDRDMA
x3200c0s1b0n0:199893:200114 [3] NCCL INFO Channel 00/0 : 7[3] -> 6[2] via P2P/CUMEM/read
x3200c0s1b0n0:199893:200114 [3] NCCL INFO Channel 02/0 : 7[3] -> 6[2] via P2P/CUMEM/read
x3200c0s1b0n0:199893:200114 [3] NCCL INFO Connected all rings
x3200c0s1b0n0:199893:200117 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199893:200114 [3] NCCL INFO Channel 01/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/1/GDRDMA
x3200c0s1b0n0:199893:200117 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199893:200114 [3] NCCL INFO Channel 03/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/1/GDRDMA
x3200c0s1b0n0:199893:200114 [3] NCCL INFO Channel 00/0 : 7[3] -> 4[0] via P2P/CUMEM/read
x3200c0s1b0n0:199893:200114 [3] NCCL INFO Channel 02/0 : 7[3] -> 4[0] via P2P/CUMEM/read
x3200c0s1b0n0:199893:200114 [3] NCCL INFO Channel 01/0 : 7[3] -> 63] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM/read
x3200c0s19b1n0:190245:190466 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM/read
x3200c0s19b1n0:190245:190466 [3] NCCL INFO Connected all trees
x3200c0s19b1n0:190245:190466 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3200c0s19b1n0:190245:190466 [3] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x3200c0s19b1n0:190245:190466 [3] NCCL INFO comm 0x5630808a4380 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId c7000 commId 0x5f4dce13ead548e7 - Init COMPLETE
[rank3]:[E ProcessGroupNCCL.cpp:563] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120040 milliseconds before timing out.
[2] via P2P/CUMEM/read
x3200c0s1b0n0:199893:200114 [3] NCCL INFO Channel 03/0 : 7[3] -> 6[2] via P2P/CUMEM/read
x3200c0s1b0n0:199893:200114 [3] NCCL INFO Connected all trees
x3200c0s1b0n0:199893:200114 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3200c0s1b0n0:199893:200114 [3] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x3200c0s1b0n0:199893:200114 [3] NCCL INFO comm 0x55da73ee82e0 rank 7 nranks 8 cudaDev 3 nvmlDev 3 busId c7000 commId 0x5f4dce13ead548e7 - Init COMPLETE
[rank7]:[E ProcessGroupNCCL.cpp:563] [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120038 milliseconds before timing out.
[rank3]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 3] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 1081146416430583048.
[rank3]:[E ProcessGroupNCCL.cpp:577] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E ProcessGroupNCCL.cpp:583] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank3]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120040 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x148cc818d6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148cc92e24a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x148cc92bbb61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x148cc92bc035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x148cc92bce6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x148ce2642e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x148ced5356ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x148ced2f550f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120040 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x148cc818d6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148cc92e24a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x148cc92bbb61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x148cc92bc035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x148cc92bce6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x148ce2642e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x148ced5356ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x148ced2f550f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x148cc818d6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148cc92e24a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x148cc8f7cd40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x148ce2642e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x148ced5356ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x148ced2f550f in /lib64/libc.so.6)

[rank7]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 7] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 0.
[rank7]:[E ProcessGroupNCCL.cpp:577] [Rank 7] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank7]:[E ProcessGroupNCCL.cpp:583] [Rank 7] To avoid data inconsistency, we are taking the entire process down.
[rank7]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120038 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x153d495c76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x153d4a71c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x153d4a6f5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x153d4a6f6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x153d4a6f6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x153d63a42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x153d6e9876ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x153d6e74750f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120038 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x153d495c76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x153d4a71c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x153d4a6f5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x153d4a6f6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x153d4a6f6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x153d63a42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x153d6e9876ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x153d6e74750f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x153d495c76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x153d4a71c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x153d4a3b6d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x153d63a42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x153d6e9876ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x153d6e74750f in /lib64/libc.so.6)

x3200c0s1b0n0:199894:199894 [2] NCCL INFO cudaDriverVersion 12020
x3200c0s1b0n0:199894:199894 [2] NCCL INFO Bootstrap : Using bond0:10.140.48.173<0>
x3200c0s1b0n0:199894:199894 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
x3200c0s1b0n0:199894:199894 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
x3200c0s1b0n0:199894:200116 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s1b0n0:199894:200116 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s1b0n0:199894:200116 [2] NCCL INFO NET/OFI Using Libfabric version 1.15
x3200c0s1b0n0:199894:200116 [2] NCCL INFO NET/OFI Using CUDA driver version 12020
x3200c0s1b0n0:199894:200116 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
x3200c0s1b0n0:199894:200116 [2] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s1b0n0:199894:200116 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199894:200116 [2] NCCL INFO Using non-device net plugin version 0
x3200c0s1b0n0:199894:200116 [2] NCCL INFO Using network AWS Libfabric
x3200c0s1b0n0:199894:200116 [2] NCCL INFO DMA-BUF is available on GPU device 2
x3200c0s1b0n0:199894:200116 [2] NCCL INFO comm 0x5597e95d1d20 rank 6 nranks 8 cudaDev 2 nvmlDev 2 busId 85000 commId 0x5f4dce13ead548e7 - Init START
x3200c0s1b0n0:199894:200116 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199894:200116 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199894:200116 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199894:200116 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199894:200116 [2] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s1b0n0:199894:200116 [2] NCCL INFO Setting affinity for GPU 2 to ff00,00000000
x3200c0s1b0n0:199894:200116 [2] NCCL INFO NVLS multicast support is not available on d[rank6]:[E ProcessGroupNCCL.cpp:563] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120050 milliseconds before timing out.
ev 2
x3200c0s1b0n0:199894:200116 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
x3200c0s1b0n0:199894:200116 [2] NCCL INFO comm 0x5597e95d1d20 rank 6 nRanks 8 nNodes 2 localRanks 4 localRank 2 MNNVL 0
x3200c0s1b0n0:199894:200116 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 4/-1/-1->6->7 [2] 7/-1/-1->6->5 [3] 4/-1/-1->6->7
x3200c0s1b0n0:199894:200116 [2] NCCL INFO P2P Chunksize set to 131072
x3200c0s1b0n0:199894:200116 [2] NCCL INFO Channel 01/0 : 6[2] -> 7[3] via P2P/CUMEM/read
x3200c0s1b0n0:199894:200116 [2] NCCL INFO Channel 03/0 : 6[2] -> 7[3] via P2P/CUMEM/read
x3200c0s1b0n0:199894:200116 [2] NCCL INFO Channel 00/0 : 6[2] -> 4[0] via P2P/CUMEM/read
x3200c0s1b0n0:199894:200116 [2] NCCL INFO Channel 02/0 : 6[2] -> 4[0] via P2P/CUMEM/read
x3200c0s1b0n0:199894:200116 [2] NCCL INFO Connected all rings
x3200c0s1b0n0:199894:200116 [2] NCCL INFO Channel 00/0 : 6[2] -> 7[3] via P2P/CUMEM/read
x3200c0s1b0n0:199894:200116 [2] NCCL INFO Channel 02/0 : 6[2] -> 7[3] via P2P/CUMEM/read
x3200c0s1b0n0:199894:200116 [2] NCCL INFO Channel 01/0 : 6[2] -> 4[0] via P2P/CUMEM/read
x3200c0s1b0n0:199894:200116 [2] NCCL INFO Channel 03/0 : 6[2] -> 4[0] via P2P/CUMEM/read
x3200c0s1b0n0:199894:200116 [2] NCCL INFO Channel 00/0 : 6[2] -> 5[1] via P2P/CUMEM/read
x3200c0s1b0n0:199894:200116 [2] NCCL INFO Channel 02/0 : 6[2] -> 5[1] via P2P/CUMEM/read
x3200c0s1b0n0:199894:200116 [2] NCCL INFO Connected all trees
x3200c0s1b0n0:199894:200116 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3200c0s1b0n0:199894:200116 [2] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x3200c0s1b0n0:199894:200116 [2] NCCL INFO comm 0x5597e95d1d20 rank 6 nranks 8 cudaDev 2 nvmlDev 2 busId 85000 commId 0x5f4dce13ead548e7 - Init COMPLETE
[rank6]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 6] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 0.
[rank6]:[E ProcessGroupNCCL.cpp:577] [Rank 6] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank6]:[E ProcessGroupNCCL.cpp:583] [Rank 6] To avoid data inconsistency, we are taking the entire process down.
[rank6]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 6] Process group watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120050 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d28ea616f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d28fb1c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d28faf5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d28faf6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d28faf6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d2a8e42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d2b3e376ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d2b3bf750f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 6] Process group watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120050 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d28ea616f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d28fb1c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d28faf5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d28faf6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d28faf6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d2a8e42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d2b3e376ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d2b3bf750f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d28ea616f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d28fb1c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14d28f7b6d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14d2a8e42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14d2b3e376ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14d2b3bf750f in /lib64/libc.so.6)

x3200c0s19b1n0:190244:190464 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s19b1n0:190244:190464 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s19b1n0:190244:190464 [0] NCCL INFO NET/OFI Using Libfabric version 1.15
x3200c0s19b1n0:190244:190464 [0] NCCL INFO NET/OFI Using CUDA driver version 12020
x3200c0s19b1n0:190244:190464 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
x3200c0s19b1n0:190244:190464 [0] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s19b1n0:190244:190464 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190244:190464 [0] NCCL INFO Using non-device net plugin version 0
x3200c0s19b1n0:190244:190464 [0] NCCL INFO Using network AWS Libfabric
x3200c0s19b1n0:190244:190464 [0] NCCL INFO DMA-BUF is available on GPU device 0
x3200c0s19b1n0:190244:190464 [0] NCCL INFO comm 0x55d2e70a6f30 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 7000 commId 0x5f4dce13ead548e7 - Init START
x3200c0s19b1n0:190244:190464 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190244:190464 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190244:190464 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190244:190464 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190244:190464 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s19b1n0:190244:190464 [0] NCCL INFO NVLS multicast support is not available on dev 0
x3200c0s19b1n0:190244:190464 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
x3200c0s19b1n0:190244:190464 [0] NCCL INFO comm 0x55d2e70a6f30 rank 0 nRanks 8 nNodes 2 localRanks 4 localRank 0 MNNVL 0
x3200c0s19b1n0:190244:190464 [0] NCCL INFO Channel 00/04 :    0   2   3   7   6   4   5   1
x3200c0s19b1n0:190244:190464 [0] NCCL INFO Channel 01/04 :    0   1   5   4   6   7   3   2
x3200c0s19b1n0:190244:190464 [0] NCCL INFO Channel 02/04 :    0   2   3   7   6   4  [rank0]:[E ProcessGroupNCCL.cpp:563] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120063 milliseconds before timing out.
 5   1
x3200c0s19b1n0:190244:190464 [0] NCCL INFO Channel 03/04 :    0   1   5   4   6   7   3   2
x3200c0s19b1n0:190244:190464 [0] NCCL INFO Trees [0] -1/-1/-1->0->3 [1] 1/-1/-1->0->2 [2] -1/-1/-1->0->3 [3] 1/-1/-1->0->2
x3200c0s19b1n0:190244:190464 [0] NCCL INFO P2P Chunksize set to 131072
x3200c0s19b1n0:190244:190464 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read
x3200c0s19b1n0:190244:190464 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM/read
x3200c0s19b1n0:190244:190464 [0] NCCL INFO Channel 00/0 : 0[0] -> 2[2] via P2P/CUMEM/read
x3200c0s19b1n0:190244:190464 [0] NCCL INFO Channel 02/0 : 0[0] -> 2[2] via P2P/CUMEM/read
x3200c0s19b1n0:190244:190464 [0] NCCL INFO Connected all rings
x3200c0s19b1n0:190244:190464 [0] NCCL INFO Channel 01/0 : 0[0] -> 2[2] via P2P/CUMEM/read
x3200c0s19b1n0:190244:190464 [0] NCCL INFO Channel 03/0 : 0[0] -> 2[2] via P2P/CUMEM/read
x3200c0s19b1n0:190244:190464 [0] NCCL INFO Channel 00/0 : 0[0] -> 3[3] via P2P/CUMEM/read
x3200c0s19b1n0:190244:190464 [0] NCCL INFO Channel 02/0 : 0[0] -> 3[3] via P2P/CUMEM/read
x3200c0s19b1n0:190244:190464 [0] NCCL INFO Connected all trees
x3200c0s19b1n0:190244:190464 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3200c0s19b1n0:190244:190464 [0] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x3200c0s19b1n0:190244:190464 [0] NCCL INFO comm 0x55d2e70a6f30 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 7000 commId 0x5f4dce13ead548e7 - Init COMPLETE
[rank0]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 0] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 707378544457353688.
[rank0]:[E ProcessGroupNCCL.cpp:577] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E ProcessGroupNCCL.cpp:583] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120063 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14ac824b76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ac8360c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14ac835e5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14ac835e6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14ac835e6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14ac9cef0e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14aca78456ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14aca760550f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120063 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14ac824b76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ac8360c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14ac835e5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14ac835e6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14ac835e6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14ac9cef0e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14aca78456ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14aca760550f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14ac824b76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14ac8360c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14ac832a6d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14ac9cef0e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14aca78456ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14aca760550f in /lib64/libc.so.6)

x3200c0s19b1n0:190243:190243 [2] NCCL INFO cudaDriverVersion 12020
x3200c0s19b1n0:190243:190243 [2] NCCL INFO Bootstrap : Using bond0:10.140.48.179<0>
x3200c0s19b1n0:190243:190243 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
x3200c0s19b1n0:190243:190243 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
x3200c0s19b1n0:190243:190467 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s19b1n0:190243:190467 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s19b1n0:190243:190467 [2] NCCL INFO NET/OFI Using Libfabric version 1.15
x3200c0s19b1n0:190243:190467 [2] NCCL INFO NET/OFI Using CUDA driver version 12020
x3200c0s19b1n0:190243:190467 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
x3200c0s19b1n0:190243:190467 [2] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s19b1n0:190243:190467 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190243:190467 [2] NCCL INFO Using non-device net plugin version 0
x3200c0s19b1n0:190243:190467 [2] NCCL INFO Using network AWS Libfabric
x3200c0s19b1n0:190243:190467 [2] NCCL INFO DMA-BUF is available on GPU device 2
x3200c0s19b1n0:190243:190467 [2] NCCL INFO comm 0x56413b4e9c90 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 85000 commId 0x5f4dce13ead548e7 - Init START
x3200c0s19b1n0:190243:190467 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190243:190467 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190243:190467 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190243:190467 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:190243:190467 [2] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s19b1n0:190243:190467 [2] NCCL INFO Setting affinity for GPU 2 to ff00,00000000
x3200c0s19b1n0:190243:190467 [2] NCCL INFO NVLS multicast support[rank2]:[E ProcessGroupNCCL.cpp:563] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120068 milliseconds before timing out.
 is not available on dev 2
x3200c0s19b1n0:190243:190467 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
x3200c0s19b1n0:190243:190467 [2] NCCL INFO comm 0x56413b4e9c90 rank 2 nRanks 8 nNodes 2 localRanks 4 localRank 2 MNNVL 0
x3200c0s19b1n0:190243:190467 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 0/-1/-1->2->3 [2] 3/-1/-1->2->1 [3] 0/-1/-1->2->3
x3200c0s19b1n0:190243:190467 [2] NCCL INFO P2P Chunksize set to 131072
x3200c0s19b1n0:190243:190467 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM/read
x3200c0s19b1n0:190243:190467 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM/read
x3200c0s19b1n0:190243:190467 [2] NCCL INFO Channel 01/0 : 2[2] -> 0[0] via P2P/CUMEM/read
x3200c0s19b1n0:190243:190467 [2] NCCL INFO Channel 03/0 : 2[2] -> 0[0] via P2P/CUMEM/read
x3200c0s19b1n0:190243:190467 [2] NCCL INFO Connected all rings
x3200c0s19b1n0:190243:190467 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM/read
x3200c0s19b1n0:190243:190467 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM/read
x3200c0s19b1n0:190243:190467 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM/read
x3200c0s19b1n0:190243:190467 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM/read
x3200c0s19b1n0:190243:190467 [2] NCCL INFO Connected all trees
x3200c0s19b1n0:190243:190467 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3200c0s19b1n0:190243:190467 [2] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x3200c0s19b1n0:190243:190467 [2] NCCL INFO comm 0x56413b4e9c90 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 85000 commId 0x5f4dce13ead548e7 - Init COMPLETE
[rank2]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 2] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 1081146416430583048.
[rank2]:[E ProcessGroupNCCL.cpp:577] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E ProcessGroupNCCL.cpp:583] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120068 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15088f6616f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15089071c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1508906f5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1508906f6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1508906f6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1508a9a42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1508b4a386ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1508b47f850f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120068 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15088f6616f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15089071c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1508906f5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1508906f6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1508906f6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1508a9a42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1508b4a386ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1508b47f850f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15088f6616f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15089071c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1508903b6d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1508a9a42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1508b4a386ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1508b47f850f in /lib64/libc.so.6)

abric/3/GDRDMA
x3200c0s1b0n0:199895:200115 [1] NCCL INFO Connected all trees
x3200c0s1b0n0:199895:200115 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3200c0s1b0n0:199895:200115 [1] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x3200c0s1b0n0:199895:200115 [1] NCCL INFO comm 0x55a7d6c40580 rank 5 nranks 8 cudaDev 1 nvmlDev 1 busId 46000 commId 0x5f4dce13ead548e7 - Init COMPLETE
[rank5]:[E ProcessGroupNCCL.cpp:563] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120077 milliseconds before timing out.
[rank5]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 5] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 0.
[rank5]:[E ProcessGroupNCCL.cpp:577] [Rank 5] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank5]:[E ProcessGroupNCCL.cpp:583] [Rank 5] To avoid data inconsistency, we are taking the entire process down.
[rank5]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 5] Process group watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120077 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1529a27c76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1529a391c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1529a38f5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1529a38f6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1529a38f6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1529bcc42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1529c7b426ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1529c790250f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 5] Process group watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120077 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1529a27c76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1529a391c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1529a38f5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1529a38f6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1529a38f6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1529bcc42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1529c7b426ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1529c790250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1529a27c76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1529a391c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1529a35b6d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1529bcc42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1529c7b426ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1529c790250f in /lib64/libc.so.6)

 1[1] -> 5[1] [send] via NET/AWS Libfabric/3/GDRDMA
x3200c0s19b1n0:190246:190465 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read
x3200c0s19b1n0:190246:190465 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read
x3200c0s19b1n0:190246:190465 [1] NCCL INFO Connected all trees
x3200c0s19b1n0:190246:190465 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3200c0s19b1n0:190246:190465 [1] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x3200c0s19b1n0:190246:190465 [1] NCCL INFO comm 0x559a428825d0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 46000 commId 0x5f4dce13ead548e7 - Init COMPLETE
[rank1]:[E ProcessGroupNCCL.cpp:563] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120097 milliseconds before timing out.
[rank1]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 1] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 1081146416430583048.
[rank1]:[E ProcessGroupNCCL.cpp:577] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E ProcessGroupNCCL.cpp:583] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120097 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c4915c76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c49271c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14c4926f5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14c4926f6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14c4926f6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14c4aba42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14c4b69966ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14c4b675650f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120097 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c4915c76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c49271c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14c4926f5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14c4926f6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14c4926f6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14c4aba42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14c4b69966ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14c4b675650f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c4915c76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c49271c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14c4923b6d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14c4aba42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14c4b69966ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14c4b675650f in /lib64/libc.so.6)

x3200c0s1b0n0:199896:199896 [0] NCCL INFO cudaDriverVersion 12020
x3200c0s1b0n0:199896:199896 [0] NCCL INFO Bootstrap : Using bond0:10.140.48.173<0>
x3200c0s1b0n0:199896:199896 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
x3200c0s1b0n0:199896:199896 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
x3200c0s1b0n0:199896:200113 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s1b0n0:199896:200113 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s1b0n0:199896:200113 [0] NCCL INFO NET/OFI Using Libfabric version 1.15
x3200c0s1b0n0:199896:200113 [0] NCCL INFO NET/OFI Using CUDA driver version 12020
x3200c0s1b0n0:199896:200113 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
x3200c0s1b0n0:199896:200113 [0] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s1b0n0:199896:200113 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199896:200113 [0] NCCL INFO Using non-device net plugin version 0
x3200c0s1b0n0:199896:200113 [0] NCCL INFO Using network AWS Libfabric
x3200c0s1b0n0:199896:200113 [0] NCCL INFO DMA-BUF is available on GPU device 0
x3200c0s1b0n0:199896:200113 [0] NCCL INFO comm 0x561b69964ff0 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 7000 commId 0x5f4dce13ead548e7 - Init START
x3200c0s1b0n0:199896:200113 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199896:200113 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199896:200113 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199896:200113 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:199896:200113 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s1b0n0:199896:200113 [0] NCCL INFO NVLS multicast support is not available on dev 0
x3200c0s1b0n0:199896:200113 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
[rank4]:[E ProcessGroupNCCL.cpp:563] [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120098 milliseconds before timing out.
x3200c0s1b0n0:199896:200113 [0] NCCL INFO comm 0x561b69964ff0 rank 4 nRanks 8 nNodes 2 localRanks 4 localRank 0 MNNVL 0
x3200c0s1b0n0:199896:200113 [0] NCCL INFO Trees [0] -1/-1/-1->4->7 [1] 5/-1/-1->4->6 [2] -1/-1/-1->4->7 [3] 5/-1/-1->4->6
x3200c0s1b0n0:199896:200113 [0] NCCL INFO P2P Chunksize set to 131072
x3200c0s1b0n0:199896:200113 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[1] via P2P/CUMEM/read
x3200c0s1b0n0:199896:200113 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[1] via P2P/CUMEM/read
x3200c0s1b0n0:199896:200113 [0] NCCL INFO Channel 01/0 : 4[0] -> 6[2] via P2P/CUMEM/read
x3200c0s1b0n0:199896:200113 [0] NCCL INFO Channel 03/0 : 4[0] -> 6[2] via P2P/CUMEM/read
x3200c0s1b0n0:199896:200113 [0] NCCL INFO Connected all rings
x3200c0s1b0n0:199896:200113 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[1] via P2P/CUMEM/read
x3200c0s1b0n0:199896:200113 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[1] via P2P/CUMEM/read
x3200c0s1b0n0:199896:200113 [0] NCCL INFO Channel 00/0 : 4[0] -> 7[3] via P2P/CUMEM/read
x3200c0s1b0n0:199896:200113 [0] NCCL INFO Channel 02/0 : 4[0] -> 7[3] via P2P/CUMEM/read
x3200c0s1b0n0:199896:200113 [0] NCCL INFO Connected all trees
x3200c0s1b0n0:199896:200113 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3200c0s1b0n0:199896:200113 [0] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x3200c0s1b0n0:199896:200113 [0] NCCL INFO comm 0x561b69964ff0 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 7000 commId 0x5f4dce13ead548e7 - Init COMPLETE
[rank4]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 4] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 1081146416430583048.
[rank4]:[E ProcessGroupNCCL.cpp:577] [Rank 4] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank4]:[E ProcessGroupNCCL.cpp:583] [Rank 4] To avoid data inconsistency, we are taking the entire process down.
[rank4]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 4] Process group watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120098 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1546e9a616f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1546eab1c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1546eaaf5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1546eaaf6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1546eaaf6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x154703e42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15470ee006ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15470ebc050f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 4] Process group watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120098 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1546e9a616f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1546eab1c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1546eaaf5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1546eaaf6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1546eaaf6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x154703e42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15470ee006ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15470ebc050f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1546e9a616f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1546eab1c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1546ea7b6d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x154703e42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x15470ee006ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x15470ebc050f in /lib64/libc.so.6)

./launcher.sh: line 14: 190244 Aborted                 (core dumped) $@
x3200c0s19b1n0.hsn.cm.sirius.alcf.anl.gov: rank 0 exited with code 134
x3200c0s19b1n0.hsn.cm.sirius.alcf.anl.gov: rank 2 died from signal 15
