I am 2 of 8: 2 on x3200c0s13b0n0 - MASTER_ADDR=x3200c0s13b0n0
I am 1 of 8: 1 on x3200c0s13b0n0 - MASTER_ADDR=x3200c0s13b0n0
I am 3 of 8: 3 on x3200c0s13b0n0 - MASTER_ADDR=x3200c0s13b0n0
I am 0 of 8: 0 on x3200c0s13b0n0 - MASTER_ADDR=x3200c0s13b0n0
I am 4 of 8: 0 on x3200c0s13b1n0 - MASTER_ADDR=x3200c0s13b0n0
I am 6 of 8: 2 on x3200c0s13b1n0 - MASTER_ADDR=x3200c0s13b0n0
I am 7 of 8: 3 on x3200c0s13b1n0 - MASTER_ADDR=x3200c0s13b0n0
I am 5 of 8: 1 on x3200c0s13b1n0 - MASTER_ADDR=x3200c0s13b0n0
x3200c0s13b0n0:3607805:3607805 [0] NCCL INFO Bootstrap : Using bond0:10.140.48.176<0>
x3200c0s13b0n0:3607805:3607805 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
x3200c0s13b0n0:3607805:3607805 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
x3200c0s13b0n0:3607805:3607805 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.18.3+cuda12.2
x3200c0s13b1n0:1875927:1875927 [3] NCCL INFO cudaDriverVersion 12020
x3200c0s13b1n0:1875927:1875927 [3] NCCL INFO Bootstrap : Using bond0:10.140.48.177<0>
x3200c0s13b1n0:1875927:1875927 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
x3200c0s13b1n0:1875927:1875927 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.6.0
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s13b1n0:1875927:1876333 [3] 638.142523 nccl_net_ofi_init:1395 NCCL TRACE NET/OFI Provider cxi does not require registration of local memory buffers
x3200c0s13b1n0:1875927:1876333 [3] 638.152582 nccl_net_ofi_init:1405 NCCL TRACE NET/OFI Provider cxi does not use remote virtual addressing
x3200c0s13b1n0:1875927:1876333 [3] 638.153814 nccl_net_ofi_init:1424 NCCL TRACE NET/OFI Provider cxi selects memory registration keys
x3200c0s13b1n0:1875927:1876333 [3] 638.154816 nccl_net_ofi_init:1458 NCCL TRACE NET/OFI Provider cxi requires endpoint memory registration
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO Using network AWS Libfabric
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO DMA-BUF is available on GPU device 3
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO comm 0x56252daa6930 rank 7 nranks 8 cudaDev 3 nvmlDev 3 busId c7000 commId 0xaf575d1b2709e3ba - Init START
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO NVLS multicast support is not available on dev 3
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 6/-1/-1->7->3 [2] -1/-1/-1->7->6 [3] 6/3/-1->7->-1
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO P2P Chunksize set to 131072
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO Channel 00/0 : 7[3] -> 1[1] [send] via NET/AWS Libfabric/1/GDRDMA
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO Channel 02/0 : 7[3] -> 1[1] [send] via NET/AWS Libfabric/1/GDRDMA
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO Channel 01/0 : 1[1] -> 7[3] [receive] via NET/AWS Libfabric/1/GDRDMA
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO Channel 03/0 : 1[1] -> 7[3] [receive] via NET/AWS Libfabric/1/GDRDMA
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO Channel 01/0 : 7[3] -> 6[2] via P2P/IPC/read
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO Channel 03/0 : 7[3] -> 6[2] via P2P/IPC/read
x3200c0s13b1n0:1875927:1876336 [3] 1177.384164 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875927:1876336 [3] 1180.880008 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875927:1876336 [3] 1182.374679 alloc_and_reg_flush_buff:2216 NCCL TRACE NET/OFI Registering buffer for flush operations
x3200c0s13b1n0:1875927:1876336 [3] 1182.382293 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875927:1876336 [3] 1182.734054 alloc_and_reg_flush_buff:2216 NCCL TRACE NET/OFI Registering buffer for flush operations
x3200c0s13b1n0:1875927:1876336 [3] 1182.740396 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO Connected all rings
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO Channel 01/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/1/GDRDMA
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO Channel 03/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/1/GDRDMA
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO Channel 01/0 : 7[3] -> 3[3] [send] via NET/AWS Libfabric/1/GDRDMA
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO Channel 03/0 : 7[3] -> 3[3] [send] via NET/AWS Libfabric/1/GDRDMA
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO Channel 00/0 : 7[3] -> 6[2] via P2P/IPC/read
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO Channel 02/0 : 7[3] -> 6[2] via P2P/IPC/read
x3200c0s13b1n0:1875927:1876336 [3] 1190.058317 alloc_and_reg_flush_buff:2216 NCCL TRACE NET/Ox3200c0s13b0n0:3607804:3607804 [3] NCCL INFO cudaDriverVersion 12020
x3200c0s13b0n0:3607804:3607804 [3] NCCL INFO Bootstrap : Using bond0:10.140.48.176<0>
x3200c0s13b0n0:3607804:3607804 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
x3200c0s13b0n0:3607804:3607804 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.6.0
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s13b0n0:3607804:3608249 [3] 626.385359 nccl_net_ofi_init:1395 NCCL TRACE NET/OFI Provider cxi does not require registration of local memory buffers
x3200c0s13b0n0:3607804:3608249 [3] 626.393915 nccl_net_ofi_init:1405 NCCL TRACE NET/OFI Provider cxi does not use remote virtual addressing
x3200c0s13b0n0:3607804:3608249 [3] 626.395127 nccl_net_ofi_init:1424 NCCL TRACE NET/OFI Provider cxi selects memory registration keys
x3200c0s13b0n0:3607804:3608249 [3] 626.396179 nccl_net_ofi_init:1458 NCCL TRACE NET/OFI Provider cxi requires endpoint memory registration
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO Using network AWS Libfabric
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO DMA-BUF is available on GPU device 3
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO comm 0x5610a5e2bff0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId c7000 commId 0xaf575d1b2709e3ba - Init START
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO NVLS multicast support is not available on dev 3
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 2/7/-1->3->-1 [2] -1/-1/-1->3->2 [3] 2/-1/-1->3->7
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO P2P Chunksize set to 131072
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO Channel 00/0 : 3[3] -> 5[1] [send] via NET/AWS Libfabric/1/GDRDMA
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO Channel 02/0 : 3[3] -> 5[1] [send] via NET/AWS Libfabric/1/GDRDMA
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO Channel 01/0 : 5[1] -> 3[3] [receive] via NET/AWS Libfabric/1/GDRDMA
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO Channel 03/0 : 5[1] -> 3[3] [receive] via NET/AWS Libfabric/1/GDRDMA
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC/read
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/IPC/read
x3200c0s13b0n0:3607804:3608251 [3] 1197.203995 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607804:3608251 [3] 1201.261690 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607804:3608251 [3] 1202.589131 alloc_and_reg_flush_buff:2216 NCCL TRACE NET/OFI Registering buffer for flush operations
x3200c0s13b0n0:3607804:3608251 [3] 1202.596555 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607804:3608251 [3] 1205.115282 alloc_and_reg_flush_buff:2216 NCCL TRACE NET/OFI Registering buffer for flush operations
x3200c0s13b0n0:3607804:3608251 [3] 1205.121163 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO Connected all rings
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO Channel 01/0 : 7[3] -> 3[3] [receive] via NET/AWS Libfabric/1/GDRDMA
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO Channel 03/0 : 7[3] -> 3[3] [receive] via NET/AWS Libfabric/1/GDRDMA
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO Channel 01/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/1/GDRDMA
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO Channel 03/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/1/GDRDMA
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC/read
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC/read
x3200c0s13b0n0:3607804:3608251 [3] 1212.869675 register_mr_buffers:584 NCCL TRACE NET/OFI Skix3200c0s13b1n0:1875928:1875928 [1] NCCL INFO cudaDriverVersion 12020
x3200c0s13b1n0:1875928:1875928 [1] NCCL INFO Bootstrap : Using bond0:10.140.48.177<0>
x3200c0s13b1n0:1875928:1875928 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
x3200c0s13b1n0:1875928:1875928 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.6.0
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s13b1n0:1875928:1876334 [1] 635.716722 nccl_net_ofi_init:1395 NCCL TRACE NET/OFI Provider cxi does not require registration of local memory buffers
x3200c0s13b1n0:1875928:1876334 [1] 635.727362 nccl_net_ofi_init:1405 NCCL TRACE NET/OFI Provider cxi does not use remote virtual addressing
x3200c0s13b1n0:1875928:1876334 [1] 635.728694 nccl_net_ofi_init:1424 NCCL TRACE NET/OFI Provider cxi selects memory registration keys
x3200c0s13b1n0:1875928:1876334 [1] 635.729686 nccl_net_ofi_init:1458 NCCL TRACE NET/OFI Provider cxi requires endpoint memory registration
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO Using network AWS Libfabric
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO DMA-BUF is available on GPU device 1
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO comm 0x55df42bd43d0 rank 5 nranks 8 cudaDev 1 nvmlDev 1 busId 46000 commId 0xaf575d1b2709e3ba - Init START
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO Setting affinity for GPU 1 to ff0000
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO NVLS multicast support is not available on dev 1
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 4/-1/-1->5->6 [2] 6/-1/-1->5->4 [3] 4/-1/-1->5->6
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO P2P Chunksize set to 131072
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO Channel 00/0 : 3[3] -> 5[1] [receive] via NET/AWS Libfabric/3/GDRDMA
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO Channel 02/0 : 3[3] -> 5[1] [receive] via NET/AWS Libfabric/3/GDRDMA
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO Channel 01/0 : 5[1] -> 3[3] [send] via NET/AWS Libfabric/3/GDRDMA
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO Channel 03/0 : 5[1] -> 3[3] [send] via NET/AWS Libfabric/3/GDRDMA
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO Channel 00/0 : 5[1] -> 4[0] via P2P/IPC/read
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO Channel 02/0 : 5[1] -> 4[0] via P2P/IPC/read
x3200c0s13b1n0:1875928:1876337 [1] 1174.316986 alloc_and_reg_flush_buff:2216 NCCL TRACE NET/OFI Registering buffer for flush operations
x3200c0s13b1n0:1875928:1876337 [1] 1174.328147 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875928:1876337 [1] 1180.212603 alloc_and_reg_flush_buff:2216 NCCL TRACE NET/OFI Registering buffer for flush operations
x3200c0s13b1n0:1875928:1876337 [1] 1180.222852 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875928:1876337 [1] 1182.573042 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875928:1876337 [1] 1183.159044 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO Connected all rings
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO Channel 00/0 : 5[1] -> 6[2] via P2P/IPC/read
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO Channel 01/0 : 5[1] -> 6[2] via P2P/IPC/read
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO Channel 02/0 : 5[1] -> 6[2] via P2P/IPC/read
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO Channel 03/0 : 5[1] -> 6[2] via P2P/IPC/read
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO Channel 01/0 : 5[1] -> 4[0] via P2P/IPC/read
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO Channel 03/0 : 5[1] -> 4[0] via P2P/IPC/read
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO Connected all trees
x3200c0s13b1n0:1875928:1876334 [1] NCCx3200c0s13b0n0:3607803:3607803 [1] NCCL INFO cudaDriverVersion 12020
x3200c0s13b0n0:3607803:3607803 [1] NCCL INFO Bootstrap : Using bond0:10.140.48.176<0>
x3200c0s13b0n0:3607803:3607803 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
x3200c0s13b0n0:3607803:3607803 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.6.0
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s13b0n0:3607803:3608250 [1] 644.980596 nccl_net_ofi_init:1395 NCCL TRACE NET/OFI Provider cxi does not require registration of local memory buffers
x3200c0s13b0n0:3607803:3608250 [1] 644.989162 nccl_net_ofi_init:1405 NCCL TRACE NET/OFI Provider cxi does not use remote virtual addressing
x3200c0s13b0n0:3607803:3608250 [1] 644.990455 nccl_net_ofi_init:1424 NCCL TRACE NET/OFI Provider cxi selects memory registration keys
x3200c0s13b0n0:3607803:3608250 [1] 644.991547 nccl_net_ofi_init:1458 NCCL TRACE NET/OFI Provider cxi requires endpoint memory registration
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO Using network AWS Libfabric
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO DMA-BUF is available on GPU device 1
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO comm 0x55e0004ceba0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 46000 commId 0xaf575d1b2709e3ba - Init START
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO Setting affinity for GPU 1 to ff0000
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO NVLS multicast support is not available on dev 1
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 0/-1/-1->1->2 [2] 2/-1/-1->1->0 [3] 0/-1/-1->1->2
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO P2P Chunksize set to 131072
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO Channel 00/0 : 7[3] -> 1[1] [receive] via NET/AWS Libfabric/3/GDRDMA
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO Channel 02/0 : 7[3] -> 1[1] [receive] via NET/AWS Libfabric/3/GDRDMA
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO Channel 01/0 : 1[1] -> 7[3] [send] via NET/AWS Libfabric/3/GDRDMA
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO Channel 03/0 : 1[1] -> 7[3] [send] via NET/AWS Libfabric/3/GDRDMA
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC/read
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC/read
x3200c0s13b0n0:3607803:3608253 [1] 1191.482647 alloc_and_reg_flush_buff:2216 NCCL TRACE NET/OFI Registering buffer for flush operations
x3200c0s13b0n0:3607803:3608253 [1] 1191.493437 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607803:3608253 [1] 1197.528303 alloc_and_reg_flush_buff:2216 NCCL TRACE NET/OFI Registering buffer for flush operations
x3200c0s13b0n0:3607803:3608253 [1] 1197.536979 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607803:3608253 [1] 1200.919076 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607803:3608253 [1] 1201.475470 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO Connected all rings
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC/read
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC/read
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/IPC/read
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC/read
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC/read
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC/read
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO Connected all trees
x3200c0s13b0n0:3607803:3608250 [1] NCCx3200c0s13b1n0:1875925:1875925 [0] NCCL INFO cudaDriverVersion 12020
x3200c0s13b1n0:1875925:1875925 [0] NCCL INFO Bootstrap : Using bond0:10.140.48.177<0>
x3200c0s13b1n0:1875925:1875925 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
x3200c0s13b1n0:1875925:1875925 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.6.0
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s13b1n0:1875925:1876332 [0] 749.918818 nccl_net_ofi_init:1395 NCCL TRACE NET/OFI Provider cxi does not require registration of local memory buffers
x3200c0s13b1n0:1875925:1876332 [0] 749.927604 nccl_net_ofi_init:1405 NCCL TRACE NET/OFI Provider cxi does not use remote virtual addressing
x3200c0s13b1n0:1875925:1876332 [0] 749.928907 nccl_net_ofi_init:1424 NCCL TRACE NET/OFI Provider cxi selects memory registration keys
x3200c0s13b1n0:1875925:1876332 [0] 749.929858 nccl_net_ofi_init:1458 NCCL TRACE NET/OFI Provider cxi requires endpoint memory registration
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO Using network AWS Libfabric
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO DMA-BUF is available on GPU device 0
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO comm 0x556973d5d180 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 7000 commId 0xaf575d1b2709e3ba - Init START
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO NVLS multicast support is not available on dev 0
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO Trees [0] 5/-1/-1->4->0 [1] -1/-1/-1->4->5 [2] 5/0/-1->4->-1 [3] -1/-1/-1->4->5
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO P2P Chunksize set to 131072
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[1] via P2P/IPC/read
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[1] via P2P/IPC/read
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO Channel 00/0 : 4[0] -> 6[2] via P2P/IPC/read
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO Channel 02/0 : 4[0] -> 6[2] via P2P/IPC/read
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO Connected all rings
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[1] via P2P/IPC/read
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[1] via P2P/IPC/read
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO Channel 00/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/3
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO Channel 02/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/3
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO Channel 00/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/3
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO Channel 02/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/3
x3200c0s13b1n0:1875925:1876339 [0] 1816.109924 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875925:1876339 [0] 1816.116055 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875925:1876339 [0] 1816.117638 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875925:1876339 [0] 1816.161060 alloc_and_reg_flush_buff:2216 NCCL TRACE NET/OFI Registering buffer for flush operations
x3200c0s13b1n0:1875925:1876339 [0] 1816.168684 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875925:1876339 [0] 1821.844337 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875925:1876339 [0] 1821.848084 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875925:1876339 [0] 1821.849136 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875925:1876339 [0] 1827.482129 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.6.0
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s13b0n0:3607805:3608247 [0] 737.531949 nccl_net_ofi_init:1395 NCCL TRACE NET/OFI Provider cxi does not require registration of local memory buffers
x3200c0s13b0n0:3607805:3608247 [0] 737.536768 nccl_net_ofi_init:1405 NCCL TRACE NET/OFI Provider cxi does not use remote virtual addressing
x3200c0s13b0n0:3607805:3608247 [0] 737.538051 nccl_net_ofi_init:1424 NCCL TRACE NET/OFI Provider cxi selects memory registration keys
x3200c0s13b0n0:3607805:3608247 [0] 737.539062 nccl_net_ofi_init:1458 NCCL TRACE NET/OFI Provider cxi requires endpoint memory registration
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO Using network AWS Libfabric
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO DMA-BUF is available on GPU device 0
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO comm 0x55b8ab134940 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 7000 commId 0xaf575d1b2709e3ba - Init START
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO NVLS multicast support is not available on dev 0
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO Channel 00/04 :    0   2   3   5   4   6   7   1
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO Channel 01/04 :    0   1   7   6   4   5   3   2
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO Channel 02/04 :    0   2   3   5   4   6   7   1
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO Channel 03/04 :    0   1   7   6   4   5   3   2
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO Trees [0] 1/4/-1->0->-1 [1] -1/-1/-1->0->1 [2] 1/-1/-1->0->4 [3] -1/-1/-1->0->1
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO P2P Chunksize set to 131072
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC/read
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC/read
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO Channel 00/0 : 0[0] -> 2[2] via P2P/IPC/read
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO Channel 02/0 : 0[0] -> 2[2] via P2P/IPC/read
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO Connected all rings
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC/read
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC/read
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO Channel 00/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/3
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO Channel 02/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/3
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO Channel 00/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/3
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO Channel 02/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/3
x3200c0s13b0n0:3607805:3608254 [0] 1831.204975 alloc_and_reg_flush_buff:2216 NCCL TRACE NET/OFI Registering buffer for flush operations
x3200c0s13b0n0:3607805:3608254 [0] 1831.217128 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607805:3608254 [0] 1837.276730 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607805:3608254 [0] 1837.280296 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607805:3608254 [0] 1837.281579 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607805:3608254 [0] 1842.842956 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607805:3608254 [0] 1842.846603 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607805:3608254 [0] 1842.847675 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607805:3608254 [0] 1848.486157 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. lx3200c0s13b1n0:1875925:1876339 [0] 1827.485976 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875925:1876339 [0] 1827.493681 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875925:1876339 [0] 1827.524308 alloc_and_reg_flush_buff:2216 NCCL TRACE NET/OFI Registering buffer for flush operations
x3200c0s13b1n0:1875925:1876339 [0] 1827.532454 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875925:1876339 [0] 1833.344633 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875925:1876339 [0] 1833.347910 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875925:1876339 [0] 1833.348952 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO Connected all trees
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x3200c0s13b1n0:1875925:1876332 [0] NCCL INFO comm 0x556973d5d180 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 7000 commId 0xaf575d1b2709e3ba - Init COMPLETE
[rank4]:[E ProcessGroupNCCL.cpp:563] [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120008 milliseconds before timing out.
[rank4]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 4] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 5405991176141820966.
[rank4]:[E ProcessGroupNCCL.cpp:577] [Rank 4] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank4]:[E ProcessGroupNCCL.cpp:583] [Rank 4] To avoid data inconsistency, we are taking the entire process down.
[rank4]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 4] Process group watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120008 milliseconds before timing out.
Exception raised from checkTimeout at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x147a0e654729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x147a0f72e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x147a0f707a01 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x147a0f707ed5 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x147a0f708d0d in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x147a27e42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x147a2b8b26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x147a2b67250f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 4] Process group watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120008 milliseconds before timing out.
Exception raised from checkTimeout at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x147a0e654729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x147a0f72e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x147a0f707a01 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x147a0f707ed5 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x147a0f708d0d in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x147a27e42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x147a2b8b26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x147a2b67250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x147a0e654729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x147a0f72e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd1402a (0x147a0f3cc02a in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x147a27e42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x147a2b8b26ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x147a2b67250f in /lib64/libc.so.6)

ocal_mr: 0
x3200c0s13b0n0:3607805:3608254 [0] 1848.496426 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607805:3608254 [0] 1848.497558 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607805:3608254 [0] 1848.560186 alloc_and_reg_flush_buff:2216 NCCL TRACE NET/OFI Registering buffer for flush operations
x3200c0s13b0n0:3607805:3608254 [0] 1848.566137 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607805:3608254 [0] 1854.448146 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607805:3608254 [0] 1854.451342 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607805:3608254 [0] 1854.452724 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO Connected all trees
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x3200c0s13b0n0:3607805:3608247 [0] NCCL INFO comm 0x55b8ab134940 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 7000 commId 0xaf575d1b2709e3ba - Init COMPLETE
[rank0]:[E ProcessGroupNCCL.cpp:563] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120012 milliseconds before timing out.
L INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x3200c0s13b1n0:1875928:1876334 [1] NCCL INFO comm 0x55df42bd43d0 rank 5 nranks 8 cudaDev 1 nvmlDev 1 busId 46000 commId 0xaf575d1b2709e3ba - Init COMPLETE
[rank5]:[E ProcessGroupNCCL.cpp:563] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120015 milliseconds before timing out.
[rank0]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 0] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 23069509474624.
[rank0]:[E ProcessGroupNCCL.cpp:577] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E ProcessGroupNCCL.cpp:583] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120012 milliseconds before timing out.
Exception raised from checkTimeout at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14fc5b0aa729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x14fc5c21e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14fc5c1f7a01 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14fc5c1f7ed5 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14fc5c1f8d0d in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14fc74a42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14fc7838e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14fc7814e50f in /lib64/libc.so.6)

[rank5]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 5] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 358394888192.
[rank5]:[E ProcessGroupNCCL.cpp:577] [Rank 5] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [rank5]:[E ProcessGroupNCCL.cpp:583] [Rank 5] To avoid data inconsistency, we are taking the entire process down.
[rank5]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 5] Process group watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120015 milliseconds before timing out.
Exception raised from checkTimeout at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c7a8749729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x14c7a98bd321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14c7a9896a01 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14c7a9896ed5 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14c7a9897d0d in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14c7c2042e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14c7c59976ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14c7c575750f in /lib64/libc.so.6)

[PG 0 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120012 milliseconds before timing out.
Exception raised from checkTimeout at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14fc5b0aa729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x14fc5c21e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14fc5c1f7a01 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14fc5c1f7ed5 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14fc5c1f8d0d in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14fc74a42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14fc7838e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14fc7814e50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14fc5b0aa729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x14fc5c21e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd1402a (0x14fc5bebc02a in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14fc74a42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14fc7838e6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14fc7814e50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 5] Process group watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120015 milliseconds before timing out.
Exception raised from checkTimeout at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c7a8749729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x14c7a98bd321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14c7a9896a01 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14c7a9896ed5 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14c7a9897d0d in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14c7c2042e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14c7c59976ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14c7c575750f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c7a8749729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x14c7a98bd321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd1402a (0x14c7a955b02a in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14c7c2042e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14c7c59976ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14c7c575750f in /lib64/libc.so.6)

p registering host buffer. local_mr: 0
x3200c0s13b0n0:3607804:3608251 [3] 1214.584022 alloc_and_reg_flush_buff:2216 NCCL TRACE NET/OFI Registering buffer for flush operations
x3200c0s13b0n0:3607804:3608251 [3] 1214.591696 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607804:3608251 [3] 1215.667385 alloc_and_reg_flush_buff:2216 NCCL TRACE NET/OFI Registering buffer for flush operations
x3200c0s13b0n0:3607804:3608251 [3] 1215.673156 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607804:3608251 [3] 1219.277360 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO Connected all trees
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x3200c0s13b0n0:3607804:3608249 [3] NCCL INFO comm 0x5610a5e2bff0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId c7000 commId 0xaf575d1b2709e3ba - Init COMPLETE
[rank3]:[E ProcessGroupNCCL.cpp:563] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120030 milliseconds before timing out.
[rank3]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 3] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 8315178084276987250.
[rank3]:[E ProcessGroupNCCL.cpp:577] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E ProcessGroupNCCL.cpp:583] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank3]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120030 milliseconds before timing out.
Exception raised from checkTimeout at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x146a2e4aa729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x146a2f61e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146a2f5f7a01 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x146a2f5f7ed5 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x146a2f5f8d0d in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146a482f0e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x146a4b7476ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x146a4b50750f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120030 milliseconds before timing out.
Exception raised from checkTimeout at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x146a2e4aa729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x146a2f61e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x146a2f5f7a01 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x146a2f5f7ed5 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x146a2f5f8d0d in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146a482f0e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x146a4b7476ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x146a4b50750f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x146a2e4aa729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x146a2f61e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd1402a (0x146a2f2bc02a in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x146a482f0e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x146a4b7476ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x146a4b50750f in /lib64/libc.so.6)

FI Registering buffer for flush operations
x3200c0s13b1n0:1875927:1876336 [3] 1190.070089 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875927:1876336 [3] 1193.025557 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875927:1876336 [3] 1197.123884 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875927:1876336 [3] 1197.280990 alloc_and_reg_flush_buff:2216 NCCL TRACE NET/OFI Registering buffer for flush operations
x3200c0s13b1n0:1875927:1876336 [3] 1197.286570 register_mr_buffers:584 NCCL TRACE NET/OFI Skip registering host buffer. local_mr: 0
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO Connected all trees
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x3200c0s13b1n0:1875927:1876333 [3] NCCL INFO comm 0x56252daa6930 rank 7 nranks 8 cudaDev 3 nvmlDev 3 busId c7000 commId 0xaf575d1b2709e3ba - Init COMPLETE
[rank7]:[E ProcessGroupNCCL.cpp:563] [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120045 milliseconds before timing out.
[rank7]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 7] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 358394888192.
[rank7]:[E ProcessGroupNCCL.cpp:577] [Rank 7] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank7]:[E ProcessGroupNCCL.cpp:583] [Rank 7] To avoid data inconsistency, we are taking the entire process down.
[rank7]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120045 milliseconds before timing out.
Exception raised from checkTimeout at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14f9a6e54729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x14f9a7f2e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14f9a7f07a01 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14f9a7f07ed5 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14f9a7f08d0d in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14f9c0642e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14f9c41036ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14f9c3ec350f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120045 milliseconds before timing out.
Exception raised from checkTimeout at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14f9a6e54729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x14f9a7f2e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14f9a7f07a01 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14f9a7f07ed5 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14f9a7f08d0d in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14f9c0642e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14f9c41036ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14f9c3ec350f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14f9a6e54729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x14f9a7f2e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd1402a (0x14f9a7bcc02a in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14f9c0642e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14f9c41036ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14f9c3ec350f in /lib64/libc.so.6)

x3200c0s13b1n0:1875926:1875926 [2] NCCL INFO cudaDriverVersion 12020
x3200c0s13b1n0:1875926:1875926 [2] NCCL INFO Bootstrap : Using bond0:10.140.48.177<0>
x3200c0s13b1n0:1875926:1875926 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
x3200c0s13b1n0:1875926:1875926 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.6.0
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s13b1n0:1875926:1876335 [2] 628.853485 nccl_net_ofi_init:1395 NCCL TRACE NET/OFI Provider cxi does not require registration of local memory buffers
x3200c0s13b1n0:1875926:1876335 [2] 628.863944 nccl_net_ofi_init:1405 NCCL TRACE NET/OFI Provider cxi does not use remote virtual addressing
x3200c0s13b1n0:1875926:1876335 [2] 628.865157 nccl_net_ofi_init:1424 NCCL TRACE NET/OFI Provider cxi selects memory registration keys
x3200c0s13b1n0:1875926:1876335 [2] 628.866119 nccl_net_ofi_init:1458 NCCL TRACE NET/OFI Provider cxi requires endpoint memory registration
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO Using network AWS Libfabric
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO DMA-BUF is available on GPU device 2
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO comm 0x55ecf0cb2ae0 rank 6 nranks 8 cudaDev 2 nvmlDev 2 busId 85000 commId 0xaf575d1b2709e3ba - Init START
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO Setting affinity for GPU 2 to ff00,00000000
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO NVLS multicast support is not available on dev 2
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 5/-1/-1->6->7 [2] 7/-1/-1->6->5 [3] 5/-1/-1->6->7
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO P2P Chunksize set to 131072
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO Channel 00/0 : 6[2] -> 7[3] via P2P/I[rank6]:[E ProcessGroupNCCL.cpp:563] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120084 milliseconds before timing out.
PC/read
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO Channel 02/0 : 6[2] -> 7[3] via P2P/IPC/read
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO Channel 01/0 : 6[2] -> 4[0] via P2P/IPC/read
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO Channel 03/0 : 6[2] -> 4[0] via P2P/IPC/read
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO Connected all rings
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO Channel 01/0 : 6[2] -> 7[3] via P2P/IPC/read
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO Channel 03/0 : 6[2] -> 7[3] via P2P/IPC/read
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO Channel 00/0 : 6[2] -> 5[1] via P2P/IPC/read
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO Channel 01/0 : 6[2] -> 5[1] via P2P/IPC/read
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO Channel 02/0 : 6[2] -> 5[1] via P2P/IPC/read
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO Channel 03/0 : 6[2] -> 5[1] via P2P/IPC/read
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO Connected all trees
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x3200c0s13b1n0:1875926:1876335 [2] NCCL INFO comm 0x55ecf0cb2ae0 rank 6 nranks 8 cudaDev 2 nvmlDev 2 busId 85000 commId 0xaf575d1b2709e3ba - Init COMPLETE
[rank6]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 6] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 358394888192.
[rank6]:[E ProcessGroupNCCL.cpp:577] [Rank 6] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank6]:[E ProcessGroupNCCL.cpp:583] [Rank 6] To avoid data inconsistency, we are taking the entire process down.
[rank6]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 6] Process group watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120084 milliseconds before timing out.
Exception raised from checkTimeout at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x153ca57ba729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x153ca692e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x153ca6907a01 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x153ca6907ed5 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x153ca6908d0d in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x153cbf042e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x153cc2a3f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x153cc27ff50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 6] Process group watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120084 milliseconds before timing out.
Exception raised from checkTimeout at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x153ca57ba729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x153ca692e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x153ca6907a01 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x153ca6907ed5 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x153ca6908d0d in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x153cbf042e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x153cc2a3f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x153cc27ff50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x153ca57ba729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x153ca692e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd1402a (0x153ca65cc02a in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x153cbf042e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x153cc2a3f6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x153cc27ff50f in /lib64/libc.so.6)

x3200c0s13b0n0:3607802:3607802 [2] NCCL INFO cudaDriverVersion 12020
x3200c0s13b0n0:3607802:3607802 [2] NCCL INFO Bootstrap : Using bond0:10.140.48.176<0>
x3200c0s13b0n0:3607802:3607802 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
x3200c0s13b0n0:3607802:3607802 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.6.0
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s13b0n0:3607802:3608248 [2] 635.381271 nccl_net_ofi_init:1395 NCCL TRACE NET/OFI Provider cxi does not require registration of local memory buffers
x3200c0s13b0n0:3607802:3608248 [2] 635.390949 nccl_net_ofi_init:1405 NCCL TRACE NET/OFI Provider cxi does not use remote virtual addressing
x3200c0s13b0n0:3607802:3608248 [2] 635.392361 nccl_net_ofi_init:1424 NCCL TRACE NET/OFI Provider cxi selects memory registration keys
x3200c0s13b0n0:3607802:3608248 [2] 635.393263 nccl_net_ofi_init:1458 NCCL TRACE NET/OFI Provider cxi requires endpoint memory registration
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO Using network AWS Libfabric
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO DMA-BUF is available on GPU device 2
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO comm 0x560e73384e00 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 85000 commId 0xaf575d1b2709e3ba - Init START
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO Setting affinity for GPU 2 to ff00,00000000
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO NVLS multicast support is not available on dev 2
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 1/-1/-1->2->3 [2] 3/-1/-1->2->1 [3] 1/-1/-1->2->3
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO P2P Chunksize set to 131072
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/I[rank2]:[E ProcessGroupNCCL.cpp:563] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120092 milliseconds before timing out.
PC/read
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC/read
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO Channel 01/0 : 2[2] -> 0[0] via P2P/IPC/read
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO Channel 03/0 : 2[2] -> 0[0] via P2P/IPC/read
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO Connected all rings
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC/read
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/IPC/read
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC/read
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC/read
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC/read
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC/read
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO Connected all trees
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x3200c0s13b0n0:3607802:3608248 [2] NCCL INFO comm 0x560e73384e00 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 85000 commId 0xaf575d1b2709e3ba - Init COMPLETE
[rank2]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 2] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 358394888192.
[rank2]:[E ProcessGroupNCCL.cpp:577] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E ProcessGroupNCCL.cpp:583] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120092 milliseconds before timing out.
Exception raised from checkTimeout at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14fe98bba729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x14fe99d2e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14fe99d07a01 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14fe99d07ed5 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14fe99d08d0d in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14feb2442e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14feb5e906ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14feb5c5050f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120092 milliseconds before timing out.
Exception raised from checkTimeout at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14fe98bba729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x14fe99d2e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14fe99d07a01 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14fe99d07ed5 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14fe99d08d0d in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14feb2442e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14feb5e906ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14feb5c5050f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14fe98bba729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x14fe99d2e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd1402a (0x14fe999cc02a in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14feb2442e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14feb5e906ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14feb5c5050f in /lib64/libc.so.6)

L INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x3200c0s13b0n0:3607803:3608250 [1] NCCL INFO comm 0x55e0004ceba0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 46000 commId 0xaf575d1b2709e3ba - Init COMPLETE
[rank1]:[E ProcessGroupNCCL.cpp:563] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120096 milliseconds before timing out.
[rank1]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 1] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 5405991176141820966.
[rank1]:[E ProcessGroupNCCL.cpp:577] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E ProcessGroupNCCL.cpp:583] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120096 milliseconds before timing out.
Exception raised from checkTimeout at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x152319054729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x15231a12e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x15231a107a01 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15231a107ed5 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15231a108d0d in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152332842e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1523362a26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15233606250f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120096 milliseconds before timing out.
Exception raised from checkTimeout at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x152319054729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x15231a12e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x15231a107a01 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15231a107ed5 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15231a108d0d in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x152332842e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1523362a26ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15233606250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /lus/tegu/projects/PolarisAT/hzheng/alcf-nccl-tests/pytests/v2.3.0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x152319054729 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1076321 (0x15231a12e321 in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd1402a (0x152319dcc02a in /home/hzheng/PolarisAT/alcf-nccl-tests/pytests/venvs/torch_2.3.0.nccl_2.18/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x152332842e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1523362a26ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x15233606250f in /lib64/libc.so.6)

./launcher.sh: line 23: 3607802 Aborted                 (core dumped) $@
x3200c0s13b0n0.hsn.cm.sirius.alcf.anl.gov: rank 2 exited with code 134
x3200c0s13b0n0.hsn.cm.sirius.alcf.anl.gov: rank 0 died from signal 15
