I am 0 of 4: 0 on x3200c0s19b1n0
I am 1 of 4: 1 on x3200c0s19b1n0
I am 3 of 4: 1 on x3200c0s1b0n0
I am 2 of 4: 0 on x3200c0s1b0n0
x3200c0s19b1n0:192866:192866 [0] NCCL INFO Bootstrap : Using bond0:10.140.48.179<0>
x3200c0s19b1n0:192866:192866 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
x3200c0s19b1n0:192866:192866 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
x3200c0s19b1n0:192866:192866 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.20.5+cuda12.4
x3200c0s1b0n0:201743:201743 [0] NCCL INFO cudaDriverVersion 12020
x3200c0s1b0n0:201743:201743 [0] NCCL INFO Bootstrap : Using bond0:10.140.48.173<0>
x3200c0s1b0n0:201743:201743 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
x3200c0s1b0n0:201743:201743 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
x3200c0s1b0n0:201743:201892 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s1b0n0:201743:201892 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s1b0n0:201743:201892 [0] NCCL INFO NET/OFI Using Libfabric version 1.15
x3200c0s1b0n0:201743:201892 [0] NCCL INFO NET/OFI Using CUDA driver version 12020
x3200c0s1b0n0:201743:201892 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
x3200c0s1b0n0:201743:201892 [0] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s1b0n0:201743:201892 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Using non-device net plugin version 0
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Using network AWS Libfabric
x3200c0s1b0n0:201743:201892 [0] NCCL INFO DMA-BUF is available on GPU device 0
x3200c0s1b0n0:201743:201892 [0] NCCL INFO comm 0x55cef1af5ee0 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 7000 commId 0x9406609aa8b2f9ee - Init START
x3200c0s1b0n0:201743:201892 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s1b0n0:201743:201892 [0] NCCL INFO comm 0x55cef1af5ee0 rank 2 nRanks 4 nNodes 2 localRanks 2 localRank 0 MNNVL 0
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] 3/-1/-1->2->0 [2] -1/-1/-1->2->3 [3] -1/-1/-1->2->3 [4] 3/0/-1->2->-1 [5] 3/0/-1->2->-1 [6] -1/-1/-1->2->3 [7] -1/-1/-1->2->3
x3200c0s1b0n0:201743:201892 [0] NCCL INFO P2P Chunksize set to 131072
x3200c0s1b0n0:201743:201896 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 00/0 : 1[1] -> 2[0] [receive] via NET/AWS Libfabric/3
x3200c0s1b0n0:201743:201896 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 01/0 : 1[1] -> 2[0] [receive] via NET/AWS Libfabric/1
x3200c0s1b0n0:201743:201896 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 04/0 : 1[1] -> 2[0] [receive] via NET/AWS Libfabric/3
x3200c0s1b0n0:201743:201896 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 05/0 : 1[1] -> 2[0] [receive] via NET/AWS Libfabric/1
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[1] via P2P/CUMEM/read
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[1] via P2P/CUMEM/read
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 04/0 : 2[0] -> 3[1] via P2P/CUMEM/read
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 05/0 : 2[0] -> 3[1] via P2P/CUMEM/read
x3200c0s1b0n0:201743:201896 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 02/0 : 2[0] -> 1[1] [send] via NET/AWS Libfabric/3
x3200c0s1b0n0:201743:201896 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 03/0 : 2[0] -> 1[1] [send] via NET/AWS Libfabric/1
x3200c0s1b0n0:201743:201896 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 06/0 : 2[0] -> 1[1] [send] via NET/AWS Libfabric/3
x3200c0s1b0n0:201743:201896 [0] NCCL INFO NET/OFI Libfabric providx3200c0s19b1n0:192866:192983 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s19b1n0:192866:192983 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s19b1n0:192866:192983 [0] NCCL INFO NET/OFI Using Libfabric version 1.15
x3200c0s19b1n0:192866:192983 [0] NCCL INFO NET/OFI Using CUDA driver version 12020
x3200c0s19b1n0:192866:192983 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
x3200c0s19b1n0:192866:192983 [0] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s19b1n0:192866:192983 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Using non-device net plugin version 0
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Using network AWS Libfabric
x3200c0s19b1n0:192866:192983 [0] NCCL INFO DMA-BUF is available on GPU device 0
x3200c0s19b1n0:192866:192983 [0] NCCL INFO comm 0x5598f2929790 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 7000 commId 0x9406609aa8b2f9ee - Init START
x3200c0s19b1n0:192866:192983 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s19b1n0:192866:192983 [0] NCCL INFO comm 0x5598f2929790 rank 0 nRanks 4 nNodes 2 localRanks 2 localRank 0 MNNVL 0
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 00/08 :    0   1   2   3
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 01/08 :    0   1   2   3
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 02/08 :    0   3   2   1
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 03/08 :    0   3   2   1
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 04/08 :    0   1   2   3
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 05/08 :    0 er  1   2   3
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 06/08 :    0   3   2   1
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 07/08 :    0   3   2   1
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/2/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1 [4] 1/-1/-1->0->2 [5] 1/-1/-1->0->2 [6] -1/-1/-1->0->1 [7] -1/-1/-1->0->1
x3200c0s19b1n0:192866:192983 [0] NCCL INFO P2P Chunksize set to 131072
x3200c0s19b1n0:192866:192988 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 00/0 : 3[1] -> 0[0] [receive] via NET/AWS Libfabric/3
x3200c0s19b1n0:192866:192988 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 01/0 : 3[1] -> 0[0] [receive] via NET/AWS Libfabric/1
x3200c0s19b1n0:192866:192988 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 04/0 : 3[1] -> 0[0] [receive] via NET/AWS Libfabric/3
x3200c0s19b1n0:192866:192988 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 05/0 : 3[1] -> 0[0] [receive] via NET/AWS Libfabric/1
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM/read
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM/read
x3200c0s19b1n0:192866:192988 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 02/0 : 0[0] -> 3[1] [send] via NET/AWS Libfabric/3
x3200c0s19b1n0:192866:192988 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 03/0 : 0[0] -> 3[1] [send] via NET/AWS Libfabric/1
x3200c0s19b1n0:192866:192988 [0] x3200c0s19b1n0:192867:192867 [1] NCCL INFO cudaDriverVersion 12020
x3200c0s19b1n0:192867:192867 [1] NCCL INFO Bootstrap : Using bond0:10.140.48.179<0>
x3200c0s19b1n0:192867:192867 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
x3200c0s19b1n0:192867:192867 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
x3200c0s19b1n0:192867:192985 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s19b1n0:192867:192985 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s19b1n0:192867:192985 [1] NCCL INFO NET/OFI Using Libfabric version 1.15
x3200c0s19b1n0:192867:192985 [1] NCCL INFO NET/OFI Using CUDA driver version 12020
x3200c0s19b1n0:192867:192985 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
x3200c0s19b1n0:192867:192985 [1] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s19b1n0:192867:192985 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Using non-device net plugin version 0
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Using network AWS Libfabric
x3200c0s19b1n0:192867:192985 [1] NCCL INFO DMA-BUF is available on GPU device 1
x3200c0s19b1n0:192867:192985 [1] NCCL INFO comm 0x5556bdf04420 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 46000 commId 0x9406609aa8b2f9ee - Init START
x3200c0s19b1n0:192867:192985 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Setting affinity for GPU 1 to ff0000
x3200c0s19b1n0:192867:192985 [1] NCCL INFO comm 0x5556bdf04420 rank 1 nRanks 4 nNodes 2 localRanks 2 localRank 1 MNNVL 0
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 0/3/-1->1->-1 [3] 0/3/-1->1->-1 [4] -1/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] 0/-1/-1->1->3 [7] 0/-1/-1->1->3
x3200c0s19b1n0:192867:192985 [1] NCCL INFO P2P Chunksize set to 131072
x3200c0s19b1n0:192867:192987 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[0] [send] via NET/AWS Libfabric/3/GDRDMA
x3200c0s19b1n0:192867:192987 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[0] [send] via NET/AWS Libfabric/1
x3200c0s19b1n0:192867:192987 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[0] [send] via NET/AWS Libfabric/3/GDRDMA
x3200c0s19b1n0:192867:192987 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[0] [send] via NET/AWS Libfabric/1
x3200c0s19b1n0:192867:192987 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 02/0 : 2[0] -> 1[1] [receive] via NET/AWS Libfabric/3/GDRDMA
x3200c0s19b1n0:192867:192987 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 03/0 : 2[0] -> 1[1] [receive] via NET/AWS Libfabric/1
x3200c0s19b1n0:192867:192987 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 06/0 : 2[0] -> 1[1] [receive] via NET/AWS Libfabric/3/GDRDMA
x3200c0s19b1n0:192867:192987 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 07/0 : 2[0] -> 1[1] [receive] via NET/AWS Libfabric/1
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via Px3200c0s1b0n0:201742:201742 [1] NCCL INFO cudaDriverVersion 12020
x3200c0s1b0n0:201742:201742 [1] NCCL INFO Bootstrap : Using bond0:10.140.48.173<0>
x3200c0s1b0n0:201742:201742 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
x3200c0s1b0n0:201742:201742 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
x3200c0s1b0n0:201742:201894 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s1b0n0:201742:201894 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
x3200c0s1b0n0:201742:201894 [1] NCCL INFO NET/OFI Using Libfabric version 1.15
x3200c0s1b0n0:201742:201894 [1] NCCL INFO NET/OFI Using CUDA driver version 12020
x3200c0s1b0n0:201742:201894 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
x3200c0s1b0n0:201742:201894 [1] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
x3200c0s1b0n0:201742:201894 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Using non-device net plugin version 0
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Using network AWS Libfabric
x3200c0s1b0n0:201742:201894 [1] NCCL INFO DMA-BUF is available on GPU device 1
x3200c0s1b0n0:201742:201894 [1] NCCL INFO comm 0x557af74933a0 rank 3 nranks 4 cudaDev 1 nvmlDev 1 busId 46000 commId 0x9406609aa8b2f9ee - Init START
x3200c0s1b0n0:201742:201894 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Setting affinity for GPU 1 to ff0000
x3200c0s1b0n0:201742:201894 [1] NCCL INFO comm 0x557af74933a0 rank 3 nRanks 4 nNodes 2 localRanks 2 localRank 1 MNNVL 0
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 2/-1/-1->3->1 [3] 2/-1/-1->3->1 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] 2/1/-1->3->-1 [7] 2/1/-1->3->-1
x3200c0s1b0n0:201742:201894 [1] NCCL INFO P2P Chunksize set to 131072
x3200c0s1b0n0:201742:201897 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 00/0 : 3[1] -> 0[0] [send] via NET/AWS Libfabric/3/GDRDMA
x3200c0s1b0n0:201742:201897 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 01/0 : 3[1] -> 0[0] [send] via NET/AWS Libfabric/1
x3200c0s1b0n0:201742:201897 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 04/0 : 3[1] -> 0[0] [send] via NET/AWS Libfabric/3/GDRDMA
x3200c0s1b0n0:201742:201897 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 05/0 : 3[1] -> 0[0] [send] via NET/AWS Libfabric/1
x3200c0s1b0n0:201742:201897 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 02/0 : 0[0] -> 3[1] [receive] via NET/AWS Libfabric/3/GDRDMA
x3200c0s1b0n0:201742:201897 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 03/0 : 0[0] -> 3[1] [receive] via NET/AWS Libfabric/1
x3200c0s1b0n0:201742:201897 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 06/0 : 0[0] -> 3[1] [receive] via NET/AWS Libfabric/3/GDRDMA
x3200c0s1b0n0:201742:201897 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 07/0 : 0[0] -> 3[1] [receive] via NET/AWS Libfabric/1
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 02/0 : 3[1] -> 2[0] via P2P/CUMEM/read
x3200c0s1b0n0:201742:2018942P/CUMEM/read
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM/read
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM/read
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Connected all rings
x3200c0s19b1n0:192867:192987 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 02/0 : 3[1] -> 1[1] [receive] via NET/AWS Libfabric/3/GDRDMA
x3200c0s19b1n0:192867:192987 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 03/0 : 3[1] -> 1[1] [receive] via NET/AWS Libfabric/1
x3200c0s19b1n0:192867:192987 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 06/0 : 3[1] -> 1[1] [receive] via NET/AWS Libfabric/3/GDRDMA
x3200c0s19b1n0:192867:192987 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 07/0 : 3[1] -> 1[1] [receive] via NET/AWS Libfabric/1
x3200c0s19b1n0:192867:192987 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 02/0 : 1[1] -> 3[1] [send] via NET/AWS Libfabric/3/GDRDMA
x3200c0s19b1n0:192867:192987 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 03/0 : 1[1] -> 3[1] [send] via NET/AWS Libfabric/1
x3200c0s19b1n0:192867:192987 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 06/0 : 1[1] -> 3[1] [send] via NET/AWS Libfabric/3/GDRDMA
x3200c0s19b1n0:192867:192987 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 07/0 : 1[1] -> 3[1] [send] via NET/AWS Libfabric/1
x3200c0s19b1n0:192867:192985 [1] NCCL INFO C[rank1]:[E ProcessGroupNCCL.cpp:563] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120005 milliseconds before timing out.
hannel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM/read
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM/read
x3200c0s19b1n0:192867:192985 [1] NCCL INFO Connected all trees
x3200c0s19b1n0:192867:192985 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
x3200c0s19b1n0:192867:192985 [1] NCCL INFO 8 coll channels, 0 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
x3200c0s19b1n0:192867:192985 [1] NCCL INFO comm 0x5556bdf04420 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 46000 commId 0x9406609aa8b2f9ee - Init COMPLETE
[rank1]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 1] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 22406235116752.
[rank1]:[E ProcessGroupNCCL.cpp:577] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E ProcessGroupNCCL.cpp:583] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120005 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1460b6fc76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1460b811c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1460b80f5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1460b80f6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1460b80f6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1460d1442e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1460dc3a66ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1460dc16650f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120005 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1460b6fc76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1460b811c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1460b80f5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1460b80f6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1460b80f6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1460d1442e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1460dc3a66ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1460dc16650f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1460b6fc76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1460b811c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1460b7db6d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1460d1442e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1460dc3a66ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1460dc16650f in /lib64/libc.so.6)

 [1] NCCL INFO Channel 03/0 : 3[1] -> 2[0] via P2P/CUMEM/read
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 06/0 : 3[1] -> 2[0] via P2P/CUMEM/read
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 07/0 : 3[1] -> 2[0] via P2P/CUMEM/read
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Connected all rings
x3200c0s1b0n0:201742:201897 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 02/0 : 1[1] -> 3[1] [receive] via NET/AWS Libfabric/3/GDRDMA
x3200c0s1b0n0:201742:201897 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 03/0 : 1[1] -> 3[1] [receive] via NET/AWS Libfabric/1
x3200c0s1b0n0:201742:201897 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 06/0 : 1[1] -> 3[1] [receive] via NET/AWS Libfabric/3/GDRDMA
x3200c0s1b0n0:201742:201897 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 07/0 : 1[1] -> 3[1] [receive] via NET/AWS Libfabric/1
x3200c0s1b0n0:201742:201897 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 02/0 : 3[1] -> 1[1] [send] via NET/AWS Libfabric/3/GDRDMA
x3200c0s1b0n0:201742:201897 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 03/0 : 3[1] -> 1[1] [send] via NET/AWS Libfabric/1
x3200c0s1b0n0:201742:201897 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 06/0 : 3[1] -> 1[1] [send] via NET/AWS Libfabric/3/GDRDMA
x3200c0s1b0n0:201742:201897 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 07/0 : 3[1] -> 1[1] [send] via NET/AWS Libfabric/1
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 00/0 : 3[1] -> 2[0] via P2P/CUMEM/read
x3200c0s1b0n0:20[rank3]:[E ProcessGroupNCCL.cpp:563] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120007 milliseconds before timing out.
1742:201894 [1] NCCL INFO Channel 01/0 : 3[1] -> 2[0] via P2P/CUMEM/read
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 04/0 : 3[1] -> 2[0] via P2P/CUMEM/read
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Channel 05/0 : 3[1] -> 2[0] via P2P/CUMEM/read
x3200c0s1b0n0:201742:201894 [1] NCCL INFO Connected all trees
x3200c0s1b0n0:201742:201894 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
x3200c0s1b0n0:201742:201894 [1] NCCL INFO 8 coll channels, 0 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
x3200c0s1b0n0:201742:201894 [1] NCCL INFO comm 0x557af74933a0 rank 3 nranks 4 cudaDev 1 nvmlDev 1 busId 46000 commId 0x9406609aa8b2f9ee - Init COMPLETE
[rank3]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 3] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 0.
[rank3]:[E ProcessGroupNCCL.cpp:577] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E ProcessGroupNCCL.cpp:583] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank3]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120007 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x154558eb76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15455a00c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x154559fe5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x154559fe6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x154559fe6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1545738f0e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15457e2466ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15457e00650f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120007 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x154558eb76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15455a00c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x154559fe5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x154559fe6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x154559fe6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1545738f0e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15457e2466ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15457e00650f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x154558eb76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15455a00c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x154559ca6d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1545738f0e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x15457e2466ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x15457e00650f in /lib64/libc.so.6)

NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 06/0 : 0[0] -> 3[1] [send] via NET/AWS Libfabric/3
x3200c0s19b1n0:192866:192988 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 07/0 : 0[0] -> 3[1] [send] via NET/AWS Libfabric/1
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Connected all rings
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM/read
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM/read
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM/read
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM/read
x3200c0s19b1n0:192866:192988 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 00/0 : 2[0] -> 0[0] [receive] via NET/AWS Libfabric/3
x3200c0s19b1n0:192866:192988 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 01/0 : 2[0] -> 0[0] [receive] via NET/AWS Libfabric/1
x3200c0s19b1n0:192866:192988 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 04/0 : 2[0] -> 0[0] [receive] via NET/AWS Libfabric/3
x3200c0s19b1n0:192866:192988 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 05/0 : 2[0] -> 0[0] [receive] via NET/AWS Libfabric/1
x3200c0s19b1n0:192866:192988 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 00/0 : 0[0] -> 2[0] [send] via NET/AWS Libfabric/3
x3200c0s19b1n0:192866:192988 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 01/0 : 0[0] -> 2[0] [send] via NET/AWS Libfabric/1
x3200c0s19b1n0:192866:192988 [rank0]:[E ProcessGroupNCCL.cpp:563] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120024 milliseconds before timing out.
[0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 04/0 : 0[0] -> 2[0] [send] via NET/AWS Libfabric/3
x3200c0s19b1n0:192866:192988 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Channel 05/0 : 0[0] -> 2[0] [send] via NET/AWS Libfabric/1
x3200c0s19b1n0:192866:192983 [0] NCCL INFO Connected all trees
x3200c0s19b1n0:192866:192983 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
x3200c0s19b1n0:192866:192983 [0] NCCL INFO 8 coll channels, 0 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
x3200c0s19b1n0:192866:192983 [0] NCCL INFO comm 0x5598f2929790 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 7000 commId 0x9406609aa8b2f9ee - Init COMPLETE
[rank0]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 0] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 707378544457353688.
[rank0]:[E ProcessGroupNCCL.cpp:577] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E ProcessGroupNCCL.cpp:583] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120024 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15094f9c76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x150950b1c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x150950af5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x150950af6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x150950af6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x150969e42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x150974d646ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x150974b2450f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120024 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15094f9c76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x150950b1c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x150950af5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x150950af6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x150950af6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x150969e42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x150974d646ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x150974b2450f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15094f9c76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x150950b1c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1509507b6d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x150969e42e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x150974d646ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x150974b2450f in /lib64/libc.so.6)

 associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 07/0 : 2[0] -> 1[1] [send] via NET/AWS Libfabric/1
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Connected all rings
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[1] via P2P/CUMEM/read
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[1] via P2P/CUMEM/read
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 06/0 : 2[0] -> 3[1] via P2P/CUMEM/read
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 07/0 : 2[0] -> 3[1] via P2P/CUMEM/read
x3200c0s1b0n0:201743:201896 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 00/0 : 0[0] -> 2[0] [receive] via NET/AWS Libfabric/3
x3200c0s1b0n0:201743:201896 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 01/0 : 0[0] -> 2[0] [receive] via NET/AWS Libfabric/1
x3200c0s1b0n0:201743:201896 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 04/0 : 0[0] -> 2[0] [receive] via NET/AWS Libfabric/3
x3200c0s1b0n0:201743:201896 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 05/0 : 0[0] -> 2[0] [receive] via NET/AWS Libfabric/1
x3200c0s1b0n0:201743:201896 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 00/0 : 2[0] -> 0[0] [send] via NET/AWS Libfabric/3
x3200c0s1b0n0:201743:201896 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 01/0 : 2[0] -> 0[0] [send] via NET/AWS Libfabric/1
x3200c0s1b0n0:201743:201896 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 04/0 : 2[0] -> 0[0] [send] via NET/AWS Libfabric/3
x3200c0s1b0n0:201743:201896 [0] NCCL INFO NET/OFI Libfabric provider associates MRs w[rank2]:[E ProcessGroupNCCL.cpp:563] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120075 milliseconds before timing out.
ith endpoints
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Channel 05/0 : 2[0] -> 0[0] [send] via NET/AWS Libfabric/1
x3200c0s1b0n0:201743:201892 [0] NCCL INFO Connected all trees
x3200c0s1b0n0:201743:201892 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
x3200c0s1b0n0:201743:201892 [0] NCCL INFO 8 coll channels, 0 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
x3200c0s1b0n0:201743:201892 [0] NCCL INFO comm 0x55cef1af5ee0 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 7000 commId 0x9406609aa8b2f9ee - Init COMPLETE
[rank2]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 2] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 0.
[rank2]:[E ProcessGroupNCCL.cpp:577] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E ProcessGroupNCCL.cpp:583] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120075 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1542584616f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15425951c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1542594f5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1542594f6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1542594f6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x154272842e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15427d81f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15427d5df50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1048576, NumelOut=1048576, Timeout(ms)=120000) ran for 120075 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1542584616f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15425951c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1542594f5b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1542594f6035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1542594f6e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x154272842e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15427d81f6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15427d5df50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1542584616f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15425951c4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1542591b6d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x154272842e95 in /soft/applications/conda/2024-04-29/mconda3/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x15427d81f6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x15427d5df50f in /lib64/libc.so.6)

./launcher.sh: line 14: 192867 Aborted                 (core dumped) $@
x3200c0s19b1n0.hsn.cm.sirius.alcf.anl.gov: rank 1 exited with code 134
x3200c0s19b1n0.hsn.cm.sirius.alcf.anl.gov: rank 0 died from signal 15
